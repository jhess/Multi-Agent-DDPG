{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install .\n",
    "#!pip -q install torch\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from shutil import copyfile\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from MultiAgent import Agent\n",
    "\n",
    "env = Environment(\"/data/Tennis_Linux_NoVis/Tennis\")\n",
    "    \n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# examine the state space \n",
    "# states = env_info.vector_observations\n",
    "# state_size = states.shape[1]\n",
    "state_size = len(env_info.vector_observations[0])\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(num_agents, state_size))\n",
    "print('Action size:\\t', action_size)\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment(\"/data/Tennis_Linux_NoVis/Tennis\")\n",
    "# Place the Tennis.app file in the directory and pass \"Tennis.app\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = int(1e+5)\n",
    "batch_size = 128\n",
    "random_seed = 2\n",
    "\n",
    "num_episodes = 10000\n",
    "\n",
    "agent = Agent(num_agents, state_size, action_size, buffer_size, batch_size, random_seed)\n",
    "\n",
    "def Multi_DDPG(n_episodes=num_episodes):\n",
    "    print_every=100\n",
    "    scores_window = deque(maxlen=100)\n",
    "    scores = []\n",
    "    max_score = 0\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        states = env.reset()\n",
    "        agent.reset()\n",
    "        score = np.zeros(env.num_agents)\n",
    "        while True:\n",
    "            actions = agent.act(states) # send the action to the environment\n",
    "            # get the next state\n",
    "            next_states, rewards, dones, _ = env.step(actions)\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            score = score + rewards\n",
    "            states = next_states\n",
    "            \n",
    "            if np.any(dones):\n",
    "                break\n",
    "                \n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        avgscore_ep = np.mean(score)\n",
    "        avgscore_deque = np.mean(scores_window)\n",
    "        if i_episode % print_every == 0:\n",
    "            print(f\"Episode: {i_episode}/{num_episodes}\\tScore: {avgscore_ep:.2f}\\tAverage Score: {avgscore_deque:.2f}\")\n",
    "        if avgscore_deque>=0.5:\n",
    "            print(f\"***Solved! Episodes taken: {i_episode:d} Average Score: {avgscore_deque:.3f}***\")\n",
    "            if avgscore_deque > max_score:\n",
    "                max_score = avgscore_deque\n",
    "                torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "                torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')     \n",
    "                \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = Multi_DDPG()\n",
    "\n",
    "# got solved at 4616 episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
